This essay will focus on forms of data analytics, specifically classification methods. It will give particular focus to classification trees and clustering. This essay will give more of a focus on how they were developed and their uses more so than the mathematics they are based on. 

Cluster analysis is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups (clusters). Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value

The three forms of cluster analysis being examined in this essay is hierarchical, k-means and two step clustering. The basic process of hierarchical clustering was defined by S.C. Johnson in 1967 in his article Hierarchical Clustering Schemes in Psychometrika (Johnson, 1967). The roots of K-means clustering goes back as far as 1957 with developments from Hugo Steinhaus and Stuart Lloyd. However, the term “k-means” only came into existence in 1967, when it was used by James McQueen. (MacQueen, 1967) In 1993 Banfield and Raftery introduced a model-based distance measure for data with continuous attributes in their journal article Model-based Gaussian and non-Gaussian clustering (Banfield & Raftery, 1993). Meila and Heckerman applied this probabilistic concept and derived another distance measure for data with categorical attributes only. (Meila & Heckerman, 1998) The SPSS TwoStep Cluster Component extends this model-based distance measure to situations that include both continuous and categorical variables. (SPSS, 2001)

There are many growing tree algorithms for decision trees. The first regression tree algorithm was published in 1963 in the journal article “Problems in the analysis of survey data and a proposal” in the Journal of the American Statistical Association. The algorithm was called Automatic Interaction Detection (AID) (Morgan & Sonquist, 1963). The next advance in the field came from another journal article in Journal of the American Statistical Association published in 1972. The algorithm developed was called THeta Automatic Interaction Detection (THAID) and it extended the idea to classification (Messenger & Mandell, 1972). However, neither garnered much interest with in the statistical community. There are four decision tree growing algorithms available in SPSS C&RT, CHAID, Exhaustive CHAID and QUEST. The publication of “Classification and Regression Trees” in 1984 by Breidman et al was of paramount importance in regenerating interest in the subject, especially its development of an algorithm used in this project – C&RT (Breidman, Friedman, Olshen, & Stone, 1984). It follows the same search approach as AID and THAID but adds several improvements. The innovative pruning procedure based on the idea of weakest link cutting solved the over fitting issues of AID and THAID. Although, at the expense of computational cost (Loh, Fifthy Years of Classification and Regression Trees, 2014).CHi-squared Automatic Interaction Detector (CHAID) – an offshoot of AID had some important modifications including built in significance testing (using the most significant predictor rather than most explanatory), multi way splits and a new type of predictor for handling missing information (Kass, 1980). EXHAUSTIVE CHAID is a modification of CHAID, it explores all possible splits for each predictor. (IBM Corporation, 2012)  Quick, Unbiased and Efficient Statistical Tree (QUEST) was developed in 1997 in the journal article “SPLIT SELECTION METHODS FOR CLASSIFICATION TREES” (Loh & Shih, Split Selection Methods For Classification Trees, 1997). QUEST share many similarities with the algorithm FACT. However, it removes bias that can occur for categorical data in FACT. Furthermore, it displays substantial computational advantage over C&RT when there are categorical variables with many values (Loh, Fifthy Years of Classification and Regression Trees, 2014).
